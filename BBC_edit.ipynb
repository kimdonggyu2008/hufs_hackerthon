{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/hufs_hackerthon/blob/main/BBC_edit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBGrW2c2Ef9J",
        "outputId": "59e9b09d-c536-4c15-d25c-c5200f7a4105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVbz_C2xYoIX",
        "outputId": "5ec12b1c-af00-49c1-e08e-4e02507bda71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.16.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.16.0-py2.py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.8/313.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.16.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.3\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLarvQsWExGS",
        "outputId": "741a30e7-c29c-43e0-eee3-a64eefe07e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras.preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras.preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras.preprocessing) (1.16.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras.preprocessing\n",
            "Successfully installed keras.preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cj15fY1zEY_a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import wandb\n",
        "import os\n",
        "# print(os.listdir(\"/content/drive/MyDrive/코딩공부/dacon_project\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RZ-xexCXYrjF"
      },
      "outputs": [],
      "source": [
        "# os.environ['WANDB_API_KEY']='513a1f0c050fa7f60a76b5232e904d8df397082e'\n",
        "# os.environ['WANDB_ENTITY']='article classification'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dGjBBIEqYqgR"
      },
      "outputs": [],
      "source": [
        "# !wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jPPXl70NEls2"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/train.csv')\n",
        "# print(\"Shpe of Data\",df.shape)\n",
        "# df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fKFiMUEAEqUq"
      },
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8QHt_r8XFfZf"
      },
      "outputs": [],
      "source": [
        "# print(df['분류'].value_counts())\n",
        "# sns.countplot(data = df, x = \"분류\", palette = \"RdBu\")\n",
        "# plt.title(\"The Distribution of Category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w2Kj2GhpEsIT"
      },
      "outputs": [],
      "source": [
        "# # Show the describe include object\n",
        "# df.describe(include = 'object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qla-2kIwFzmT"
      },
      "outputs": [],
      "source": [
        "# # Lets calculate the length of the Reviews\n",
        "# df['length'] = df['제목'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y9Hkq_UIF_TD"
      },
      "outputs": [],
      "source": [
        "# plt.hist(x=df['length'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRQWJwY-Gcoi"
      },
      "source": [
        "#감정분석(필요?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BbZvij9nICb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9ab0ca-54b0-4774-e855-e2e3c35eb2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vcy4E3ITI-QN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0aa9b16-83ec-4896-94af-8392eefbd1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123621 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -qq fonts-nanum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "35872_0iH8Ci"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rcmBk_3IIRvf"
      },
      "outputs": [],
      "source": [
        "okt=Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pWaSQoSaIBjP"
      },
      "outputs": [],
      "source": [
        "def preprocess_korean(text):\n",
        "    # 특수문자 제거 및 소문자 변환\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    # 형태소 분석 후 명사만 추출\n",
        "    tokens = okt.nouns(text)\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS57fre0KOab"
      },
      "source": [
        "#카테고리를 번호로 치환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AKT_6IsRHG61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca245e4b-8f13-45db-c9b4-277cde2e08fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25267     0\n",
            "4245      0\n",
            "37931     6\n",
            "4809      0\n",
            "42775    12\n",
            "22080     0\n",
            "52347    35\n",
            "17950     0\n",
            "40895     9\n",
            "29653     1\n",
            "Name: 라벨, dtype: int64\n",
            "25267               지역\n",
            "4245                지역\n",
            "37931         정치:국회_정당\n",
            "4809                지역\n",
            "42775           경제:자동차\n",
            "22080               지역\n",
            "52347         IT_과학:과학\n",
            "17950               지역\n",
            "40895    스포츠:올림픽_아시안게임\n",
            "29653           경제:부동산\n",
            "Name: 분류, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# 카테고리 목록과 매핑될 레이블 번호를 생성합니다.\n",
        "categories = [\n",
        "    \"지역\", \"경제:부동산\", \"사회:사건_사고\", \"경제:반도체\", \"사회:사회일반\", \"사회:교육_시험\",\n",
        "    \"정치:국회_정당\", \"사회:의료_건강\", \"경제:취업_창업\", \"스포츠:올림픽_아시안게임\",\n",
        "    \"경제:산업_기업\", \"문화:전시_공연\", \"경제:자동차\", \"경제:경제일반\", \"사회:장애인\",\n",
        "    \"스포츠:골프\", \"정치:선거\", \"경제:유통\", \"IT_과학:모바일\", \"사회:여성\",\n",
        "    \"사회:노동_복지\", \"사회:환경\", \"경제:서비스_쇼핑\", \"경제:무역\", \"정치:행정_자치\",\n",
        "    \"국제\", \"문화:방송_연예\", \"스포츠:축구\", \"경제:금융_재테크\", \"정치:청와대\",\n",
        "    \"문화:출판\", \"IT_과학:IT_과학일반\", \"IT_과학:인터넷_SNS\", \"문화:미술_건축\",\n",
        "    \"정치:정치일반\", \"IT_과학:과학\", \"문화:문화일반\", \"문화:학술_문화재\", \"문화:요리_여행\",\n",
        "    \"경제:자원\", \"문화:종교\", \"IT_과학:콘텐츠\", \"사회:미디어\", \"사회:날씨\",\n",
        "    \"스포츠:농구_배구\", \"문화:음악\", \"문화:생활\", \"IT_과학:보안\", \"스포츠:월드컵\",\n",
        "    \"경제:증권_증시\", \"정치:북한\", \"정치:외교\", \"스포츠:스포츠일반\", \"문화:영화\",\n",
        "    \"스포츠:야구\", \"경제:외환\"\n",
        "]\n",
        "\n",
        "# DataFrame을 랜덤하게 섞습니다.\n",
        "shuffled = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "# 각 카테고리에 대해 데이터 샘플링\n",
        "num_of_categories = 45000\n",
        "sampled_data = []\n",
        "for category in categories:\n",
        "    sampled_data.append(shuffled[shuffled['분류'] == category][:num_of_categories])\n",
        "\n",
        "# 샘플링된 데이터를 하나의 데이터프레임으로 합칩니다.\n",
        "concated = pd.concat(sampled_data, ignore_index=True)\n",
        "\n",
        "# 데이터프레임을 다시 섞습니다.\n",
        "concated = concated.reindex(np.random.permutation(concated.index))\n",
        "\n",
        "# LABEL 열 생성 (카테고리별 레이블 부여)\n",
        "concated['라벨'] = concated['분류'].apply(lambda x: categories.index(x))\n",
        "\n",
        "# One-hot 인코딩\n",
        "labels = to_categorical(concated['라벨'], num_classes=len(categories))\n",
        "\n",
        "# 카테고리 열 삭제\n",
        "# if '분류' in concated.columns:\n",
        "#     concated = concated.drop(['분류'], axis=1)\n",
        "\n",
        "print(concated['라벨'][:10])\n",
        "print(concated['분류'][:10])\n",
        "# print(labels[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IA0pm_DwKEut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16b830b-84d3-4f17-cb9a-7aa79f2f3a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 83611 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "n_most_common_words = 20000\n",
        "max_len = 150\n",
        "tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(concated['제목'].values)\n",
        "sequences = tokenizer.texts_to_sequences(concated['제목'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jixZNw9xKjJm"
      },
      "outputs": [],
      "source": [
        "# print (\"Shape of X:\",len(X))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "up5YekkeyKIo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lpBWUyTLKp1s"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "3ndTNEyZpBBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "2BaMeZ2MyKyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwqX8qcBODnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDrex_8zOMNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 트랜스포머 블록 정의\n",
        "def transformer_block(x, head_size, num_heads, ff_dim, dropout=0.3):\n",
        "    # Multi-Head Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(x, x)\n",
        "    attention_output = Dropout(dropout)(attention_output)\n",
        "\n",
        "    # Add & Normalize\n",
        "    attention_output = Add()([x, attention_output])\n",
        "    attention_output = LayerNormalization()(attention_output)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn_output = Dense(ff_dim, activation='relu',kernel_regularizer=l2(0.001))(attention_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    ffn_output = Dense(emb_dim)(ffn_output)\n",
        "\n",
        "    # Add & Normalize\n",
        "    ffn_output = Add()([attention_output, ffn_output])\n",
        "    ffn_output = LayerNormalization()(ffn_output)\n",
        "\n",
        "    return ffn_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvxNFUFrOQYP"
      },
      "outputs": [],
      "source": [
        "emb_dim = 128\n",
        "# 입력층\n",
        "input_layer = Input(shape=(max_len,))\n",
        "\n",
        "# Embedding 레이어\n",
        "embedding_layer = Embedding(input_dim=n_most_common_words, output_dim=emb_dim, input_length=max_len)(input_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG3czHUXOy9j"
      },
      "outputs": [],
      "source": [
        "# 기본 설정\n",
        "n_most_common_words = 80000  # 어휘 수 80,000개로 설정\n",
        "emb_dim = 128  # 임베딩 벡터의 차원 설정\n",
        "max_len = X.shape[1]  # 시퀀스의 길이 (기사 제목의 길이)\n",
        "num_heads = 8  # Multi-head attention에서 헤드의 개수\n",
        "ff_dim = 128  # Feed Forward Network의 출력 차원\n",
        "num_classes = 56  # 최종 클래스의 개수\n",
        "\n",
        "# 최적화된 하이퍼파라미터\n",
        "learning_rate = 0.0005        # 학습률 (조정 가능)\n",
        "batch_size = 64             # 배치 크기\n",
        "epochs = 10                  # 최대 에포크 수\n",
        "dropout_rate = 0.7           # 드롭아웃 비율\n",
        "l2_lambda = 0.001             # L2 정규화 값\n",
        "patience = 7                  # 조기 종료를 위한 patience\n",
        "min_delta = 0.001             # 조기 종료를 위한 최소 손실 감소량\n",
        "validation_split = 0.2        # 검증 데이터 비율\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5KtTLi-OGMK"
      },
      "outputs": [],
      "source": [
        "# 트랜스포머 블록 적용\n",
        "x = transformer_block(embedding_layer, emb_dim, num_heads, ff_dim, dropout=dropout_rate)\n",
        "#x = transformer_block(x, emb_dim, num_heads, ff_dim, dropout=dropout_rate)\n",
        "# Global Average Pooling\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "# 분류를 위한 Dense 레이어 (56개의 클래스)\n",
        "output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# 모델 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh6BvicGOdkU"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCF2rJHTOend"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yYiQ3JcZSwC"
      },
      "outputs": [],
      "source": [
        "# wandb.init(\n",
        "#     # W&B 프로젝트 이름을 설정\n",
        "#     project=\"article classification\",\n",
        "\n",
        "#     # 하이퍼파라미터 및 실행 메타데이터를 추적\n",
        "#     config={\n",
        "#         \"learning_rate\": 0.001,\n",
        "#         \"architecture\": \"Transformer\",\n",
        "#         \"dataset\": \"titles\",\n",
        "#         \"epochs\": 5\n",
        "#         \"max_sequence_length\": 130,\n",
        "#         \"embedding_dim\": 128,\n",
        "#         \"num_heads\": 8,\n",
        "#         \"feedforward_dim\": 128,\n",
        "#         \"num_classes\": 56\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# \"\"\"\n",
        "# # 기본 설정\n",
        "# n_most_common_words = 80000  # 어휘 수 80,000개로 설정\n",
        "# emb_dim = 128  # 임베딩 벡터의 차원 설정\n",
        "# max_len = X.shape[1]  # 시퀀스의 길이 (기사 제목의 길이)\n",
        "# num_heads = 8  # Multi-head attention에서 헤드의 개수\n",
        "# ff_dim = 128  # Feed Forward Network의 출력 차원\n",
        "# num_classes = 56  # 최종 클래스의 개수\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 학습 결과 시각화\n",
        "def plot_training_history(history):\n",
        "    # 정확도 그래프\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # 손실 그래프\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZSZCnvo9ZV0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCV-ACaqOgxf"
      },
      "outputs": [],
      "source": [
        "# 컴파일 설정\n",
        "optimizer=AdamW(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',     # 검증 손실을 모니터링\n",
        "    mode='min',             # 'min' 모드로 설정하여, 손실이 감소하는 것을 추적\n",
        "    patience=0,             # 손실이 더 이상 개선되지 않는 즉시 학습 중단\n",
        "    restore_best_weights=True  # 가장 성능이 좋았던 가중치로 복원\n",
        ")"
      ],
      "metadata": {
        "id": "rfHq_MQHnl73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39re5JFVLIYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e80726b-e1fa-4034-9042-bc866df24947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 42ms/step - accuracy: 0.4873 - loss: 2.6278 - val_accuracy: 0.4934 - val_loss: 2.7566\n",
            "Epoch 2/10\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=validation_split,\n",
        "    callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history)"
      ],
      "metadata": {
        "id": "hm5kEkgNZNrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N7z6VIrYaKH"
      },
      "outputs": [],
      "source": [
        "# 모델 전체를 저장하는 코드 (모델 아키텍처, 가중치, 컴파일 정보 포함)\n",
        "model.save('/content/drive/MyDrive/코딩공부/dacon_project/result/bbc_edit_model.h5')  # 모델을 'model.h5' 파일로 저장\n",
        "\n",
        "print(\"모델이 'bbc_edit_model.h5'로 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEHHGtG0PT13"
      },
      "outputs": [],
      "source": [
        "accr = model.evaluate(X_test,y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjjAF5w9jMTD"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/train.csv')\n",
        "# category_counts = df['분류'].value_counts()\n",
        "\n",
        "# # 결과를 출력합니다.\n",
        "# print(category_counts[:40])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzhPTfE-PfKs"
      },
      "outputs": [],
      "source": [
        "txt = [\"용인특례시, 가을 신학기 학교 급식 공급업체 위생점검\"]\n",
        "seq = tokenizer.texts_to_sequences(txt)\n",
        "padded = pad_sequences(seq, maxlen=max_len)\n",
        "pred = model.predict(padded)\n",
        "labels = [\n",
        "    \"지역\", \"경제:부동산\", \"사회:사건_사고\", \"경제:반도체\", \"사회:사회일반\", \"사회:교육_시험\",\n",
        "    \"정치:국회_정당\", \"사회:의료_건강\", \"경제:취업_창업\", \"스포츠:올림픽_아시안게임\",\n",
        "    \"경제:산업_기업\", \"문화:전시_공연\", \"경제:자동차\", \"경제:경제일반\", \"사회:장애인\",\n",
        "    \"스포츠:골프\", \"정치:선거\", \"경제:유통\", \"IT_과학:모바일\", \"사회:여성\",\n",
        "    \"사회:노동_복지\", \"사회:환경\", \"경제:서비스_쇼핑\", \"경제:무역\", \"정치:행정_자치\",\n",
        "    \"국제\", \"문화:방송_연예\", \"스포츠:축구\", \"경제:금융_재테크\", \"정치:청와대\",\n",
        "    \"문화:출판\", \"IT_과학:IT_과학일반\", \"IT_과학:인터넷_SNS\", \"문화:미술_건축\",\n",
        "    \"정치:정치일반\", \"IT_과학:과학\", \"문화:문화일반\", \"문화:학술_문화재\", \"문화:요리_여행\",\n",
        "    \"경제:자원\", \"문화:종교\", \"IT_과학:콘텐츠\", \"사회:미디어\", \"사회:날씨\",\n",
        "    \"스포츠:농구_배구\", \"문화:음악\", \"문화:생활\", \"IT_과학:보안\", \"스포츠:월드컵\",\n",
        "    \"경제:증권_증시\", \"정치:북한\", \"정치:외교\", \"스포츠:스포츠일반\", \"문화:영화\",\n",
        "    \"스포츠:야구\", \"경제:외환\"\n",
        "]\n",
        "# labels = [ #40개 레이블만 있음\n",
        "#     \"지역\", \"경제:부동산\", \"사회:사건_사고\", \"경제:반도체\", \"사회:사회일반\", \"사회:교육_시험\",\n",
        "#     \"정치:국회_정당\", \"사회:의료_건강\", \"경제:취업_창업\", \"스포츠:올림픽_아시안게임\",\n",
        "#     \"경제:산업_기업\", \"문화:전시_공연\", \"경제:자동차\", \"경제:경제일반\", \"사회:장애인\",\n",
        "#     \"스포츠:골프\", \"정치:선거\", \"경제:유통\", \"IT_과학:모바일\", \"사회:여성\",\n",
        "#     \"사회:노동_복지\", \"사회:환경\", \"경제:서비스_쇼핑\", \"경제:무역\", \"정치:행정_자치\",\n",
        "#     \"국제\", \"문화:방송_연예\", \"스포츠:축구\", \"경제:금융_재테크\", \"정치:청와대\",\n",
        "#     \"문화:출판\", \"IT_과학:IT_과학일반\", \"IT_과학:인터넷_SNS\", \"문화:미술_건축\",\n",
        "#     \"정치:정치일반\", \"IT_과학:과학\", \"문화:문화일반\", \"문화:학술_문화재\", \"문화:요리_여행\",\n",
        "#     \"경제:자원\"\n",
        "# ]\n",
        "# print(pred, labels[np.argmax(pred)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgikRQGtYAYO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. test.csv 파일 로드\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/test.csv')\n",
        "\n",
        "# 2. 데이터 전처리: 제목을 시퀀스로 변환\n",
        "# 학습 시 사용했던 tokenizer를 사용하여 제목을 시퀀스로 변환\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['제목'].values)\n",
        "\n",
        "# 3. 시퀀스의 길이를 동일하게 맞추기 (max_len에 맞춰 패딩)\n",
        "X_test = pad_sequences(sequences_test, maxlen=max_len)\n",
        "\n",
        "# 4. 모델을 사용하여 예측\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# 5. 예측 결과에서 가장 확률이 높은 클래스를 추출\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "predicted_labels[predicted_labels>45]=0\n",
        "\n",
        "predicted_class_names=[categories[label] for label in predicted_labels]\n",
        "\n",
        "# 6. submission 데이터프레임 생성 (id와 예측된 레이블을 저장)\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': test_df['ID'],\n",
        "    '분류': predicted_class_names\n",
        "})\n",
        "\n",
        "# 7. submission.csv 파일로 저장\n",
        "submission_df.to_csv('/content/drive/MyDrive/코딩공부/dacon_project/result/submission.csv', index=False)\n",
        "\n",
        "print(\"Submission 파일이 생성되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFcqovMmgtWY"
      },
      "outputs": [],
      "source": [
        "df_test=pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/test.csv')\n",
        "df_result=pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/result/submission.csv')\n",
        "print(\"Shpe of test Data\",df_test.shape)\n",
        "print(\"Shpe of test Data\",df_result.shape)\n",
        "# df_test.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsmrlMh8k2PB"
      },
      "outputs": [],
      "source": [
        "df_result.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUhmLt9dk4Ih"
      },
      "outputs": [],
      "source": [
        "print(df_result['분류'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_result['분류'])"
      ],
      "metadata": {
        "id": "9wMlcQiKxSqy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "12tFF2Es8onbbkCASSwkDxVB5i5DgOoTZ",
      "authorship_tag": "ABX9TyO/u8ogRB/tMg4sPR+SVVVT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}