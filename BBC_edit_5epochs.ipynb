{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdonggyu2008/hufs_hackerthon/blob/main/BBC_edit_5epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBGrW2c2Ef9J",
        "outputId": "37d76794-dc81-4554-ad47-adde08516915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uVbz_C2xYoIX"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLarvQsWExGS",
        "outputId": "29d8e023-480b-4931-8145-f64c60017f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras.preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras.preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras.preprocessing) (1.16.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras.preprocessing\n",
            "Successfully installed keras.preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cj15fY1zEY_a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#import wandb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RZ-xexCXYrjF"
      },
      "outputs": [],
      "source": [
        "# os.environ['WANDB_API_KEY']='513a1f0c050fa7f60a76b5232e904d8df397082e'\n",
        "# os.environ['WANDB_ENTITY']='article classification'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dGjBBIEqYqgR"
      },
      "outputs": [],
      "source": [
        "# !wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRQWJwY-Gcoi"
      },
      "source": [
        "#감정분석(필요?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BbZvij9nICb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9330d9f6-c564-4baf-9ecc-473297fc001e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vcy4E3ITI-QN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6740617-06e1-42f5-ae82-7b784d5874cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 123621 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -qq fonts-nanum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "35872_0iH8Ci"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jPPXl70NEls2"
      },
      "outputs": [],
      "source": [
        "#augmented는 500개로 맞춤\n",
        "#augmented2는 1000개로 맞춤\n",
        "df=pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/train_augmented2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rcmBk_3IIRvf"
      },
      "outputs": [],
      "source": [
        "okt=Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pWaSQoSaIBjP"
      },
      "outputs": [],
      "source": [
        "def preprocess_korean(text):\n",
        "    # 특수문자 제거 및 소문자 변환\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    # 형태소 분석 후 명사만 추출\n",
        "    tokens = okt.nouns(text)\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS57fre0KOab"
      },
      "source": [
        "#카테고리를 번호로 치환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AKT_6IsRHG61"
      },
      "outputs": [],
      "source": [
        "# 카테고리 목록과 매핑될 레이블 번호를 생성합니다.\n",
        "categories = [\n",
        "    \"지역\", \"경제:부동산\", \"사회:사건_사고\", \"경제:반도체\", \"사회:사회일반\", \"사회:교육_시험\",\n",
        "    \"정치:국회_정당\", \"사회:의료_건강\", \"경제:취업_창업\", \"스포츠:올림픽_아시안게임\",\n",
        "    \"경제:산업_기업\", \"문화:전시_공연\", \"경제:자동차\", \"경제:경제일반\", \"사회:장애인\",\n",
        "    \"스포츠:골프\", \"정치:선거\", \"경제:유통\", \"IT_과학:모바일\", \"사회:여성\",\n",
        "    \"사회:노동_복지\", \"사회:환경\", \"경제:서비스_쇼핑\", \"경제:무역\", \"정치:행정_자치\",\n",
        "    \"국제\", \"문화:방송_연예\", \"스포츠:축구\", \"경제:금융_재테크\", \"정치:청와대\",\n",
        "    \"문화:출판\", \"IT_과학:IT_과학일반\", \"IT_과학:인터넷_SNS\", \"문화:미술_건축\",\n",
        "    \"정치:정치일반\", \"IT_과학:과학\", \"문화:문화일반\", \"문화:학술_문화재\", \"문화:요리_여행\",\n",
        "    \"경제:자원\", \"문화:종교\", \"IT_과학:콘텐츠\", \"사회:미디어\", \"사회:날씨\",\n",
        "    \"스포츠:농구_배구\", \"문화:음악\", \"문화:생활\", \"IT_과학:보안\", \"스포츠:월드컵\",\n",
        "    \"경제:증권_증시\", \"정치:북한\", \"정치:외교\", \"스포츠:스포츠일반\", \"문화:영화\",\n",
        "    \"스포츠:야구\", \"경제:외환\"\n",
        "]\n",
        "\n",
        "# DataFrame을 랜덤하게 섞습니다.\n",
        "shuffled = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "# 각 카테고리에 대해 데이터 샘플링\n",
        "num_of_categories = 45000\n",
        "sampled_data = []\n",
        "for category in categories:\n",
        "    sampled_data.append(shuffled[shuffled['분류'] == category][:num_of_categories])\n",
        "\n",
        "# 샘플링된 데이터를 하나의 데이터프레임으로 합칩니다.\n",
        "concated = pd.concat(sampled_data, ignore_index=True)\n",
        "\n",
        "# 데이터프레임을 다시 섞습니다.\n",
        "concated = concated.reindex(np.random.permutation(concated.index))\n",
        "\n",
        "# LABEL 열 생성 (카테고리별 레이블 부여)\n",
        "concated['라벨'] = concated['분류'].apply(lambda x: categories.index(x))\n",
        "\n",
        "# One-hot 인코딩\n",
        "labels = to_categorical(concated['라벨'], num_classes=len(categories))\n",
        "\n",
        "# 카테고리 열 삭제\n",
        "# if '분류' in concated.columns:\n",
        "#     concated = concated.drop(['분류'], axis=1)\n",
        "\n",
        "# print(concated['라벨'][:10])\n",
        "# print(concated['분류'])\n",
        "# print(labels[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IA0pm_DwKEut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbedc6f3-1e66-4c69-a8d3-9553b8a288ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 83611 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "n_most_common_words = 20000\n",
        "max_len = 150\n",
        "tokenizer = Tokenizer(num_words=n_most_common_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(concated['제목'].values)\n",
        "sequences = tokenizer.texts_to_sequences(concated['제목'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train = pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/train.csv')\n",
        "\n",
        "# # 각 분류별 데이터 갯수 출력\n",
        "# category_counts = train['분류'].value_counts()\n",
        "\n",
        "# # 출력\n",
        "# print(category_counts)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wWfBlvfNfjCL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#데이터 증강 및 저장(1번만)\n"
      ],
      "metadata": {
        "id": "VBe9NgTRgKPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train = pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/train.csv')\n",
        "\n",
        "# # 각 분류별 데이터 갯수 확인\n",
        "# category_counts = train['분류'].value_counts()\n",
        "\n",
        "# # 500개 이하의 데이터를 가진 분류 추출\n",
        "# under_500_categories = category_counts[category_counts < 1000].index.tolist()\n",
        "\n",
        "# # 500개 이하의 분류에 대해 데이터 증강\n",
        "# for category in under_500_categories:\n",
        "#     current_count = category_counts[category]\n",
        "#     additional_samples_needed = 1000 - current_count\n",
        "\n",
        "#     # 해당 분류의 데이터를 복제하여 증강\n",
        "#     category_data = train[train['분류'] == category]\n",
        "#     augmented_data = category_data.sample(n=additional_samples_needed, replace=True, random_state=42)\n",
        "\n",
        "#     # 원래 데이터에 증강된 데이터 추가\n",
        "#     train = pd.concat([train, augmented_data])\n",
        "\n",
        "# # 증강 후 데이터 확인\n",
        "# new_category_counts = train['분류'].value_counts()\n",
        "# print(new_category_counts)\n",
        "\n",
        "# # 증강된 데이터 저장 (선택 사항)\n",
        "# train.to_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/train_augmented2.csv', index=False)"
      ],
      "metadata": {
        "id": "ZdQax0CVf5GK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZwqX8qcBODnN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델\n"
      ],
      "metadata": {
        "id": "HmSYpyJJrnvt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iG3czHUXOy9j"
      },
      "outputs": [],
      "source": [
        "# 기본 설정\n",
        "n_most_common_words = 80000  # 어휘 수 80,000개로 설정\n",
        "emb_dim = 128  # 임베딩 벡터의 차원 설정\n",
        "max_len = X.shape[1]  # 시퀀스의 길이 (기사 제목의 길이)\n",
        "num_heads = 8  # Multi-head attention에서 헤드의 개수\n",
        "ff_dim = 128  # Feed Forward Network의 출력 차원\n",
        "num_classes = 56  # 최종 클래스의 개수\n",
        "\n",
        "# 최적화된 하이퍼파라미터\n",
        "learning_rate = 0.00005        # 학습률 (조정 가능)\n",
        "batch_size = 128             # 배치 크기\n",
        "epochs = 20                  # 최대 에포크 수\n",
        "dropout_rate = 0.8           # 드롭아웃 비율\n",
        "l2_lambda = 0.001             # L2 정규화 값\n",
        "patience = 7                  # 조기 종료를 위한 patience\n",
        "min_delta = 0.001             # 조기 종료를 위한 최소 손실 감소량\n",
        "validation_split = 0.2        # 검증 데이터 비율\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uvxNFUFrOQYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca984b3d-8767-4bad-c866-47aae4f28df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "emb_dim = 128\n",
        "# 입력층\n",
        "input_layer = Input(shape=(max_len,))\n",
        "\n",
        "# Embedding 레이어\n",
        "embedding_layer = Embedding(input_dim=n_most_common_words, output_dim=emb_dim, input_length=max_len)(input_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kDrex_8zOMNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 트랜스포머 블록 정의\n",
        "def transformer_block(x, head_size, num_heads, ff_dim, dropout=0.3):\n",
        "    # Multi-Head Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(x, x)\n",
        "    attention_output = Dropout(dropout)(attention_output)\n",
        "\n",
        "    # Add & Normalize\n",
        "    attention_output = Add()([x, attention_output])\n",
        "    attention_output = LayerNormalization()(attention_output)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn_output = Dense(ff_dim, activation='relu',kernel_regularizer=l2(0.001))(attention_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    ffn_output = Dense(emb_dim)(ffn_output)\n",
        "\n",
        "    # Add & Normalize\n",
        "    ffn_output = Add()([attention_output, ffn_output])\n",
        "    ffn_output = LayerNormalization()(ffn_output)\n",
        "\n",
        "    return ffn_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "F5KtTLi-OGMK"
      },
      "outputs": [],
      "source": [
        "# 모델 빌드 함수\n",
        "def build_model(n_most_common_words, emb_dim, max_len, num_heads, ff_dim, num_classes, dropout_rate):\n",
        "    # 입력층\n",
        "    input_layer = Input(shape=(max_len,))\n",
        "\n",
        "    # Embedding 레이어\n",
        "    embedding_layer = Embedding(input_dim=n_most_common_words, output_dim=emb_dim, input_length=max_len)(input_layer)\n",
        "\n",
        "    # 트랜스포머 블록 적용\n",
        "    x = transformer_block(embedding_layer, emb_dim, num_heads, ff_dim, dropout=dropout_rate)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Dropout (최종 분류 전에 추가)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 분류를 위한 Dense 레이어 (num_classes = 56)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # 모델 정의\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uh6BvicGOdkU"
      },
      "outputs": [],
      "source": [
        "model = build_model(n_most_common_words=80000,\n",
        "                    emb_dim=128,\n",
        "                    max_len=130,\n",
        "                    num_heads=8,\n",
        "                    ff_dim=128,\n",
        "                    num_classes=56,\n",
        "                    dropout_rate=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gCF2rJHTOend",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "834c38bb-fe85-405b-b612-2f2beb715060"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m10,240,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m527,488\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │         \u001b[38;5;34m16,512\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ layer_normalization[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │            \u001b[38;5;34m256\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ layer_normalization_1… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m)             │          \u001b[38;5;34m7,224\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,240,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">527,488</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">7,224</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,808,248\u001b[0m (41.23 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,808,248</span> (41.23 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,808,248\u001b[0m (41.23 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,808,248</span> (41.23 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7yYiQ3JcZSwC"
      },
      "outputs": [],
      "source": [
        "# wandb.init(\n",
        "#     # W&B 프로젝트 이름을 설정\n",
        "#     project=\"article classification\",\n",
        "\n",
        "#     # 하이퍼파라미터 및 실행 메타데이터를 추적\n",
        "#     config={\n",
        "#         \"learning_rate\": 0.001,\n",
        "#         \"architecture\": \"Transformer\",\n",
        "#         \"dataset\": \"titles\",\n",
        "#         \"epochs\": 5\n",
        "#         \"max_sequence_length\": 130,\n",
        "#         \"embedding_dim\": 128,\n",
        "#         \"num_heads\": 8,\n",
        "#         \"feedforward_dim\": 128,\n",
        "#         \"num_classes\": 56\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# \"\"\"\n",
        "# # 기본 설정\n",
        "# n_most_common_words = 80000  # 어휘 수 80,000개로 설정\n",
        "# emb_dim = 128  # 임베딩 벡터의 차원 설정\n",
        "# max_len = X.shape[1]  # 시퀀스의 길이 (기사 제목의 길이)\n",
        "# num_heads = 8  # Multi-head attention에서 헤드의 개수\n",
        "# ff_dim = 128  # Feed Forward Network의 출력 차원\n",
        "# num_classes = 56  # 최종 클래스의 개수\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 학습 결과 시각화\n",
        "def plot_training_history(history):\n",
        "    # 정확도 그래프\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # 손실 그래프\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZSZCnvo9ZV0l"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X , labels, test_size=0.25)"
      ],
      "metadata": {
        "id": "sU-sMBPWgRXj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CCV-ACaqOgxf"
      },
      "outputs": [],
      "source": [
        "#컴파일 설정\n",
        "optimizer=AdamW(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',     # 검증 손실을 모니터링\n",
        "    mode='min',             # 'min' 모드로 설정하여, 손실이 감소하는 것을 추적\n",
        "    patience=5,             # 손실이 더 이상 개선되지 않는 즉시 학습 중단\n",
        "    restore_best_weights=True  # 가장 성능이 좋았던 가중치로 복원\n",
        ")"
      ],
      "metadata": {
        "id": "rfHq_MQHnl73"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_weights(model):\n",
        "    # 모델의 모든 레이어의 가중치를 초기화\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'kernel_initializer') and layer.kernel_initializer:\n",
        "            layer.kernel.assign(layer.kernel_initializer(layer.kernel.shape))\n",
        "        if hasattr(layer, 'bias_initializer') and layer.bias_initializer:\n",
        "            layer.bias.assign(layer.bias_initializer(layer.bias.shape))"
      ],
      "metadata": {
        "id": "D-dng2wFJPHD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "I0SZ61DetRNR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=[5,10,15]\n",
        "learning_rates=[0.0001,0.0003,0.0005]\n",
        "batch_sizes=[64,128,256]\n",
        "dropout_rates=[0.3,0.5,0.75]\n",
        "\n",
        "best_val_loss = np.inf  # 현재 최적의 검증 손실 값\n",
        "best_val_acc=0\n",
        "best_hyperparams = {}\n",
        "\n",
        "for epoch in epochs:\n",
        "  for learning_rate in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "      for dropout_rate in dropout_rates:\n",
        "\n",
        "        print(f\"Recent epoch: {epoch}\")\n",
        "        print(f\"Recent learning_rate: {learning_rate}\")\n",
        "        print(f\"Recent batch_size: {batch_size}\")\n",
        "        print(f\"Recent dropout_rate: {dropout_rate}\")\n",
        "\n",
        "        model = build_model(n_most_common_words=n_most_common_words,\n",
        "                    emb_dim=emb_dim,\n",
        "                    max_len=max_len,\n",
        "                    num_heads=num_heads,\n",
        "                    ff_dim=ff_dim,\n",
        "                    num_classes=num_classes,\n",
        "                    dropout_rate=dropout_rate)\n",
        "\n",
        "        reset_weights(model)\n",
        "\n",
        "        optimizer=AdamW(learning_rate=learning_rate)\n",
        "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=epoch,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=validation_split,\n",
        "        callbacks=[early_stopping])\n",
        "\n",
        "\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "        val_acc = max(history.history['val_accuracy'])\n",
        "        if val_loss < best_val_loss and val_acc > best_val_acc:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_val_acc = val_acc\n",
        "                    best_hyperparams['epoch'] = epoch\n",
        "                    best_hyperparams['learning_rate'] = learning_rate\n",
        "                    best_hyperparams['batch_size'] = batch_size\n",
        "                    best_hyperparams['dropout_rate'] = dropout_rate\n",
        "\n",
        "        #plot_training_history(history)\n",
        "\n",
        "# 최종 결과 출력\n",
        "print(f\"Best val_loss: {best_val_loss}\")\n",
        "print(f\"Best val_acc: {best_val_acc}\")\n",
        "print(f\"Best Hyperparameters: Epoch: {best_hyperparams['epoch']}, \"\n",
        "      f\"Learning Rate: {best_hyperparams['learning_rate']}, \"\n",
        "      f\"Batch Size: {best_hyperparams['batch_size']}, \"\n",
        "      f\"Dropout Rate: {best_hyperparams['dropout_rate']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYiuTCHn5wLW",
        "outputId": "3ab5d21c-8950-426c-e1fb-9402f963bbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 39ms/step - accuracy: 0.2977 - loss: 3.6847 - val_accuracy: 0.3070 - val_loss: 6.0568\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 28ms/step - accuracy: 0.3064 - loss: 3.4646 - val_accuracy: 0.3160 - val_loss: 4.6470\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - accuracy: 0.4792 - loss: 2.4592 - val_accuracy: 0.7344 - val_loss: 1.2244\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.7952 - loss: 0.9361 - val_accuracy: 0.7778 - val_loss: 1.0135\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - accuracy: 0.8563 - loss: 0.6318 - val_accuracy: 0.7959 - val_loss: 0.9841\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 39ms/step - accuracy: 0.2950 - loss: 3.6733 - val_accuracy: 0.3070 - val_loss: 7.3635\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 29ms/step - accuracy: 0.3105 - loss: 3.4950 - val_accuracy: 0.3070 - val_loss: 7.6751\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.3347 - loss: 3.2588 - val_accuracy: 0.6635 - val_loss: 1.7170\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 30ms/step - accuracy: 0.7102 - loss: 1.4188 - val_accuracy: 0.7560 - val_loss: 1.2227\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.8132 - loss: 0.8634 - val_accuracy: 0.7800 - val_loss: 1.1044\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 37ms/step - accuracy: 0.2870 - loss: 3.7484 - val_accuracy: 0.3070 - val_loss: 8.6266\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 28ms/step - accuracy: 0.3095 - loss: 3.5729 - val_accuracy: 0.3070 - val_loss: 9.2971\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.3066 - loss: 3.5463 - val_accuracy: 0.3070 - val_loss: 9.7027\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 30ms/step - accuracy: 0.3049 - loss: 3.5030 - val_accuracy: 0.3533 - val_loss: 5.8062\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 29ms/step - accuracy: 0.4088 - loss: 2.7991 - val_accuracy: 0.6695 - val_loss: 1.9877\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 73ms/step - accuracy: 0.3007 - loss: 3.6870 - val_accuracy: 0.3070 - val_loss: 5.6064\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 56ms/step - accuracy: 0.3090 - loss: 3.4857 - val_accuracy: 0.3070 - val_loss: 6.2538\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.3083 - loss: 3.3969 - val_accuracy: 0.3685 - val_loss: 3.3980\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 56ms/step - accuracy: 0.5187 - loss: 2.3004 - val_accuracy: 0.7266 - val_loss: 1.2888\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 53ms/step - accuracy: 0.7916 - loss: 0.9917 - val_accuracy: 0.7711 - val_loss: 1.1057\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.2829 - loss: 3.7460 - val_accuracy: 0.3070 - val_loss: 7.1381\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - accuracy: 0.3084 - loss: 3.5180 - val_accuracy: 0.3070 - val_loss: 7.0813\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 52ms/step - accuracy: 0.3058 - loss: 3.4972 - val_accuracy: 0.3070 - val_loss: 7.1058\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 56ms/step - accuracy: 0.3088 - loss: 3.4072 - val_accuracy: 0.4158 - val_loss: 3.6595\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.5445 - loss: 2.2057 - val_accuracy: 0.7237 - val_loss: 1.3934\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 73ms/step - accuracy: 0.2807 - loss: 3.8015 - val_accuracy: 0.3070 - val_loss: 8.3388\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - accuracy: 0.3073 - loss: 3.5981 - val_accuracy: 0.3070 - val_loss: 8.2962\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 52ms/step - accuracy: 0.3077 - loss: 3.5560 - val_accuracy: 0.3070 - val_loss: 8.5736\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 56ms/step - accuracy: 0.3056 - loss: 3.5443 - val_accuracy: 0.3070 - val_loss: 9.6657\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.3093 - loss: 3.4816 - val_accuracy: 0.3116 - val_loss: 6.4047\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 154ms/step - accuracy: 0.2829 - loss: 3.7511 - val_accuracy: 0.3070 - val_loss: 3.9166\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 107ms/step - accuracy: 0.3095 - loss: 3.5092 - val_accuracy: 0.3070 - val_loss: 6.1151\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 105ms/step - accuracy: 0.3062 - loss: 3.4628 - val_accuracy: 0.3070 - val_loss: 6.1047\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 109ms/step - accuracy: 0.3024 - loss: 3.4184 - val_accuracy: 0.3070 - val_loss: 6.2074\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 110ms/step - accuracy: 0.3258 - loss: 3.2278 - val_accuracy: 0.5405 - val_loss: 2.3471\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 145ms/step - accuracy: 0.2764 - loss: 3.8217 - val_accuracy: 0.3070 - val_loss: 7.4190\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 108ms/step - accuracy: 0.3104 - loss: 3.5263 - val_accuracy: 0.3070 - val_loss: 7.6510\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 107ms/step - accuracy: 0.3060 - loss: 3.5146 - val_accuracy: 0.3070 - val_loss: 7.8196\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 108ms/step - accuracy: 0.3081 - loss: 3.4694 - val_accuracy: 0.3070 - val_loss: 7.4153\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 106ms/step - accuracy: 0.3059 - loss: 3.4095 - val_accuracy: 0.3273 - val_loss: 4.8467\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 140ms/step - accuracy: 0.2716 - loss: 3.8532 - val_accuracy: 0.3070 - val_loss: 7.8896\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 110ms/step - accuracy: 0.3069 - loss: 3.6084 - val_accuracy: 0.3070 - val_loss: 8.6167\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 109ms/step - accuracy: 0.3107 - loss: 3.5714 - val_accuracy: 0.3070 - val_loss: 8.8456\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 109ms/step - accuracy: 0.3089 - loss: 3.5569 - val_accuracy: 0.3070 - val_loss: 8.8317\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 108ms/step - accuracy: 0.3089 - loss: 3.5309 - val_accuracy: 0.3070 - val_loss: 9.5829\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 36ms/step - accuracy: 0.2966 - loss: 3.6628 - val_accuracy: 0.3070 - val_loss: 5.4140\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 29ms/step - accuracy: 0.3821 - loss: 3.0109 - val_accuracy: 0.7450 - val_loss: 1.1306\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 29ms/step - accuracy: 0.8206 - loss: 0.7557 - val_accuracy: 0.7937 - val_loss: 0.9409\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.8867 - loss: 0.4399 - val_accuracy: 0.7980 - val_loss: 0.9857\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.9212 - loss: 0.2997 - val_accuracy: 0.8050 - val_loss: 1.0129\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 38ms/step - accuracy: 0.3003 - loss: 3.6479 - val_accuracy: 0.3070 - val_loss: 7.5845\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 29ms/step - accuracy: 0.3329 - loss: 3.3079 - val_accuracy: 0.7085 - val_loss: 1.3990\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 28ms/step - accuracy: 0.7682 - loss: 1.0536 - val_accuracy: 0.7855 - val_loss: 1.0233\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - accuracy: 0.8637 - loss: 0.5595 - val_accuracy: 0.7945 - val_loss: 1.0468\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 29ms/step - accuracy: 0.8986 - loss: 0.3870 - val_accuracy: 0.8062 - val_loss: 1.0862\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 39ms/step - accuracy: 0.2974 - loss: 3.6947 - val_accuracy: 0.3070 - val_loss: 8.7547\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 30ms/step - accuracy: 0.3072 - loss: 3.5272 - val_accuracy: 0.3070 - val_loss: 8.6521\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.3241 - loss: 3.3004 - val_accuracy: 0.6245 - val_loss: 2.0326\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 28ms/step - accuracy: 0.6519 - loss: 1.6616 - val_accuracy: 0.7609 - val_loss: 1.3288\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8133 - loss: 0.8876 - val_accuracy: 0.7811 - val_loss: 1.2719\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.2957 - loss: 3.6850 - val_accuracy: 0.3070 - val_loss: 5.1854\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 56ms/step - accuracy: 0.3080 - loss: 3.4482 - val_accuracy: 0.5787 - val_loss: 2.0409\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.7033 - loss: 1.3333 - val_accuracy: 0.7815 - val_loss: 0.9645\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.8596 - loss: 0.5681 - val_accuracy: 0.7944 - val_loss: 0.9491\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 52ms/step - accuracy: 0.9023 - loss: 0.3832 - val_accuracy: 0.7898 - val_loss: 1.0465\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 72ms/step - accuracy: 0.2902 - loss: 3.6983 - val_accuracy: 0.3070 - val_loss: 7.2301\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - accuracy: 0.3063 - loss: 3.5063 - val_accuracy: 0.3070 - val_loss: 6.8742\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.4112 - loss: 2.8479 - val_accuracy: 0.7512 - val_loss: 1.1928\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.8078 - loss: 0.8520 - val_accuracy: 0.7800 - val_loss: 1.1047\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 56ms/step - accuracy: 0.8698 - loss: 0.5376 - val_accuracy: 0.7945 - val_loss: 1.1076\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 69ms/step - accuracy: 0.2936 - loss: 3.7061 - val_accuracy: 0.3070 - val_loss: 8.8754\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - accuracy: 0.3058 - loss: 3.5534 - val_accuracy: 0.3070 - val_loss: 9.6522\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - accuracy: 0.3093 - loss: 3.5002 - val_accuracy: 0.3070 - val_loss: 8.2629\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 52ms/step - accuracy: 0.3687 - loss: 2.9980 - val_accuracy: 0.7032 - val_loss: 1.6583\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 54ms/step - accuracy: 0.7179 - loss: 1.3886 - val_accuracy: 0.7683 - val_loss: 1.3612\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 149ms/step - accuracy: 0.2939 - loss: 3.7191 - val_accuracy: 0.3070 - val_loss: 5.5681\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 115ms/step - accuracy: 0.3030 - loss: 3.4894 - val_accuracy: 0.3070 - val_loss: 4.9479\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 105ms/step - accuracy: 0.3909 - loss: 2.9081 - val_accuracy: 0.7391 - val_loss: 1.2139\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 109ms/step - accuracy: 0.8060 - loss: 0.8690 - val_accuracy: 0.7861 - val_loss: 0.9891\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 109ms/step - accuracy: 0.8795 - loss: 0.5138 - val_accuracy: 0.7954 - val_loss: 0.9912\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 140ms/step - accuracy: 0.2784 - loss: 3.7653 - val_accuracy: 0.3070 - val_loss: 7.3140\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 108ms/step - accuracy: 0.3075 - loss: 3.5107 - val_accuracy: 0.3070 - val_loss: 7.6291\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 110ms/step - accuracy: 0.3054 - loss: 3.4729 - val_accuracy: 0.3070 - val_loss: 6.3285\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 108ms/step - accuracy: 0.4252 - loss: 2.7347 - val_accuracy: 0.7402 - val_loss: 1.2544\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 114ms/step - accuracy: 0.8028 - loss: 0.9100 - val_accuracy: 0.7813 - val_loss: 1.0859\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0003\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 143ms/step - accuracy: 0.2853 - loss: 3.7682 - val_accuracy: 0.3070 - val_loss: 8.2894\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 111ms/step - accuracy: 0.3050 - loss: 3.5813 - val_accuracy: 0.3070 - val_loss: 8.8317\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.3058 - loss: 3.5474 - val_accuracy: 0.3070 - val_loss: 9.9500\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 109ms/step - accuracy: 0.3099 - loss: 3.5017 - val_accuracy: 0.3070 - val_loss: 8.5586\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 105ms/step - accuracy: 0.3198 - loss: 3.3282 - val_accuracy: 0.6116 - val_loss: 2.4271\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 38ms/step - accuracy: 0.3027 - loss: 3.6428 - val_accuracy: 0.3070 - val_loss: 4.2127\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.4317 - loss: 2.7276 - val_accuracy: 0.7742 - val_loss: 0.9753\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 30ms/step - accuracy: 0.8445 - loss: 0.6116 - val_accuracy: 0.8028 - val_loss: 0.8645\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 29ms/step - accuracy: 0.9061 - loss: 0.3428 - val_accuracy: 0.8026 - val_loss: 0.9389\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 30ms/step - accuracy: 0.9323 - loss: 0.2363 - val_accuracy: 0.8074 - val_loss: 1.0008\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 36ms/step - accuracy: 0.2979 - loss: 3.6391 - val_accuracy: 0.3070 - val_loss: 7.8007\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 30ms/step - accuracy: 0.3636 - loss: 3.0904 - val_accuracy: 0.7421 - val_loss: 1.1616\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 29ms/step - accuracy: 0.8097 - loss: 0.8163 - val_accuracy: 0.7955 - val_loss: 0.9399\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.8852 - loss: 0.4446 - val_accuracy: 0.8023 - val_loss: 1.0103\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 29ms/step - accuracy: 0.9160 - loss: 0.3059 - val_accuracy: 0.8005 - val_loss: 1.1492\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 38ms/step - accuracy: 0.2979 - loss: 3.6684 - val_accuracy: 0.3070 - val_loss: 8.5948\n",
            "Epoch 2/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.3057 - loss: 3.4946 - val_accuracy: 0.3451 - val_loss: 4.5224\n",
            "Epoch 3/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.4459 - loss: 2.5361 - val_accuracy: 0.7343 - val_loss: 1.3685\n",
            "Epoch 4/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 30ms/step - accuracy: 0.7708 - loss: 1.0460 - val_accuracy: 0.7800 - val_loss: 1.2810\n",
            "Epoch 5/5\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 30ms/step - accuracy: 0.8517 - loss: 0.6408 - val_accuracy: 0.7929 - val_loss: 1.3176\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 68ms/step - accuracy: 0.2946 - loss: 3.6960 - val_accuracy: 0.3070 - val_loss: 3.4480\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 56ms/step - accuracy: 0.3166 - loss: 3.3743 - val_accuracy: 0.6849 - val_loss: 1.3906\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.7642 - loss: 0.9954 - val_accuracy: 0.7914 - val_loss: 0.9124\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - accuracy: 0.8793 - loss: 0.4486 - val_accuracy: 0.8045 - val_loss: 0.9116\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.9166 - loss: 0.2943 - val_accuracy: 0.8107 - val_loss: 0.9659\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 71ms/step - accuracy: 0.2937 - loss: 3.6884 - val_accuracy: 0.3070 - val_loss: 7.3803\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - accuracy: 0.3063 - loss: 3.4835 - val_accuracy: 0.3410 - val_loss: 4.3935\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.5497 - loss: 2.0932 - val_accuracy: 0.7743 - val_loss: 1.0291\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.8468 - loss: 0.6330 - val_accuracy: 0.7914 - val_loss: 1.0131\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.8983 - loss: 0.3890 - val_accuracy: 0.8087 - val_loss: 1.0202\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 73ms/step - accuracy: 0.2912 - loss: 3.7234 - val_accuracy: 0.3070 - val_loss: 8.8607\n",
            "Epoch 2/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.3072 - loss: 3.5299 - val_accuracy: 0.3070 - val_loss: 8.2111\n",
            "Epoch 3/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.3374 - loss: 3.1692 - val_accuracy: 0.6700 - val_loss: 1.8239\n",
            "Epoch 4/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.7028 - loss: 1.4297 - val_accuracy: 0.7687 - val_loss: 1.2718\n",
            "Epoch 5/5\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.8274 - loss: 0.7929 - val_accuracy: 0.7932 - val_loss: 1.2613\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 137ms/step - accuracy: 0.2822 - loss: 3.7748 - val_accuracy: 0.3070 - val_loss: 3.8746\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 106ms/step - accuracy: 0.3076 - loss: 3.4928 - val_accuracy: 0.3070 - val_loss: 5.3311\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 107ms/step - accuracy: 0.3070 - loss: 3.4445 - val_accuracy: 0.3070 - val_loss: 4.3857\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 106ms/step - accuracy: 0.4697 - loss: 2.4596 - val_accuracy: 0.7635 - val_loss: 1.0101\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 106ms/step - accuracy: 0.8396 - loss: 0.6518 - val_accuracy: 0.7934 - val_loss: 0.9259\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 145ms/step - accuracy: 0.2845 - loss: 3.7431 - val_accuracy: 0.3070 - val_loss: 7.0134\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 112ms/step - accuracy: 0.3094 - loss: 3.4952 - val_accuracy: 0.3070 - val_loss: 7.7589\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 106ms/step - accuracy: 0.3047 - loss: 3.4706 - val_accuracy: 0.4291 - val_loss: 3.0610\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - accuracy: 0.5939 - loss: 1.8825 - val_accuracy: 0.7601 - val_loss: 1.1077\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 106ms/step - accuracy: 0.8390 - loss: 0.6907 - val_accuracy: 0.7931 - val_loss: 1.0482\n",
            "Recent epoch: 5\n",
            "Recent learning_rate: 0.0005\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 147ms/step - accuracy: 0.2865 - loss: 3.7655 - val_accuracy: 0.3070 - val_loss: 8.7348\n",
            "Epoch 2/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 107ms/step - accuracy: 0.3094 - loss: 3.5416 - val_accuracy: 0.3070 - val_loss: 9.0524\n",
            "Epoch 3/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 112ms/step - accuracy: 0.3072 - loss: 3.5136 - val_accuracy: 0.3070 - val_loss: 10.2322\n",
            "Epoch 4/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 114ms/step - accuracy: 0.3077 - loss: 3.4294 - val_accuracy: 0.3127 - val_loss: 6.3155\n",
            "Epoch 5/5\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.3587 - loss: 2.9044 - val_accuracy: 0.6788 - val_loss: 1.7546\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 40ms/step - accuracy: 0.2934 - loss: 3.7037 - val_accuracy: 0.3070 - val_loss: 5.7253\n",
            "Epoch 2/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.3090 - loss: 3.4683 - val_accuracy: 0.3070 - val_loss: 5.7042\n",
            "Epoch 3/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.4683 - loss: 2.5404 - val_accuracy: 0.7317 - val_loss: 1.2511\n",
            "Epoch 4/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 30ms/step - accuracy: 0.7959 - loss: 0.9314 - val_accuracy: 0.7740 - val_loss: 1.0605\n",
            "Epoch 5/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.8564 - loss: 0.6255 - val_accuracy: 0.7901 - val_loss: 1.0122\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 38ms/step - accuracy: 0.2883 - loss: 3.7050 - val_accuracy: 0.3070 - val_loss: 7.3298\n",
            "Epoch 2/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 30ms/step - accuracy: 0.3062 - loss: 3.5129 - val_accuracy: 0.3070 - val_loss: 7.0574\n",
            "Epoch 3/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.3266 - loss: 3.3237 - val_accuracy: 0.6258 - val_loss: 1.9727\n",
            "Epoch 4/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 28ms/step - accuracy: 0.6956 - loss: 1.4996 - val_accuracy: 0.7506 - val_loss: 1.2380\n",
            "Epoch 5/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.8092 - loss: 0.8976 - val_accuracy: 0.7768 - val_loss: 1.1449\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 64\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 39ms/step - accuracy: 0.2876 - loss: 3.7555 - val_accuracy: 0.3070 - val_loss: 8.6222\n",
            "Epoch 2/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 28ms/step - accuracy: 0.3022 - loss: 3.5928 - val_accuracy: 0.3070 - val_loss: 8.7431\n",
            "Epoch 3/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.3046 - loss: 3.5504 - val_accuracy: 0.3070 - val_loss: 9.5630\n",
            "Epoch 4/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.3126 - loss: 3.4576 - val_accuracy: 0.4212 - val_loss: 4.0860\n",
            "Epoch 5/10\n",
            "\u001b[1m823/823\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 30ms/step - accuracy: 0.4817 - loss: 2.5003 - val_accuracy: 0.6935 - val_loss: 1.7978\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 71ms/step - accuracy: 0.2850 - loss: 3.7598 - val_accuracy: 0.3070 - val_loss: 5.3685\n",
            "Epoch 2/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 53ms/step - accuracy: 0.3060 - loss: 3.5046 - val_accuracy: 0.3070 - val_loss: 6.3934\n",
            "Epoch 3/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 56ms/step - accuracy: 0.3069 - loss: 3.4382 - val_accuracy: 0.3070 - val_loss: 6.4593\n",
            "Epoch 4/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.3837 - loss: 2.9455 - val_accuracy: 0.7075 - val_loss: 1.4280\n",
            "Epoch 5/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 55ms/step - accuracy: 0.7669 - loss: 1.1196 - val_accuracy: 0.7660 - val_loss: 1.1197\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 73ms/step - accuracy: 0.2874 - loss: 3.7319 - val_accuracy: 0.3070 - val_loss: 6.9204\n",
            "Epoch 2/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.3065 - loss: 3.5188 - val_accuracy: 0.3070 - val_loss: 7.6906\n",
            "Epoch 3/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.3076 - loss: 3.4750 - val_accuracy: 0.3070 - val_loss: 7.4961\n",
            "Epoch 4/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.3276 - loss: 3.2448 - val_accuracy: 0.5733 - val_loss: 2.3039\n",
            "Epoch 5/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.6520 - loss: 1.7236 - val_accuracy: 0.7450 - val_loss: 1.2936\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 128\n",
            "Recent dropout_rate: 0.75\n",
            "Epoch 1/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 74ms/step - accuracy: 0.2774 - loss: 3.8197 - val_accuracy: 0.3070 - val_loss: 8.1020\n",
            "Epoch 2/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 56ms/step - accuracy: 0.3056 - loss: 3.5969 - val_accuracy: 0.3070 - val_loss: 8.6129\n",
            "Epoch 3/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 53ms/step - accuracy: 0.3067 - loss: 3.5647 - val_accuracy: 0.3070 - val_loss: 9.2465\n",
            "Epoch 4/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.3037 - loss: 3.5517 - val_accuracy: 0.3070 - val_loss: 9.6217\n",
            "Epoch 5/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 54ms/step - accuracy: 0.3086 - loss: 3.4930 - val_accuracy: 0.3146 - val_loss: 8.2442\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.3\n",
            "Epoch 1/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 147ms/step - accuracy: 0.2894 - loss: 3.7701 - val_accuracy: 0.3070 - val_loss: 3.9466\n",
            "Epoch 2/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 108ms/step - accuracy: 0.3111 - loss: 3.5099 - val_accuracy: 0.3070 - val_loss: 5.7387\n",
            "Epoch 3/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 110ms/step - accuracy: 0.3010 - loss: 3.4911 - val_accuracy: 0.3070 - val_loss: 6.0119\n",
            "Epoch 4/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.3043 - loss: 3.4372 - val_accuracy: 0.3070 - val_loss: 6.1604\n",
            "Epoch 5/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 107ms/step - accuracy: 0.3132 - loss: 3.3054 - val_accuracy: 0.3634 - val_loss: 4.4581\n",
            "Recent epoch: 10\n",
            "Recent learning_rate: 0.0001\n",
            "Recent batch_size: 256\n",
            "Recent dropout_rate: 0.5\n",
            "Epoch 1/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 138ms/step - accuracy: 0.2714 - loss: 3.8440 - val_accuracy: 0.3070 - val_loss: 6.9324\n",
            "Epoch 2/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 107ms/step - accuracy: 0.3049 - loss: 3.5393 - val_accuracy: 0.3070 - val_loss: 7.1979\n",
            "Epoch 3/10\n",
            "\u001b[1m206/206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 104ms/step - accuracy: 0.3095 - loss: 3.5071 - val_accuracy: 0.3070 - val_loss: 6.9824\n",
            "Epoch 4/10\n",
            "\u001b[1m 79/206\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 101ms/step - accuracy: 0.3067 - loss: 3.4878"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 최적화된 하이퍼파라미터\n",
        "# learning_rate = 0.0005        # 학습률 (조정 가능)\n",
        "# batch_size = 128             # 배치 크기\n",
        "# epochs = 10                  # 최대 에포크 수\n",
        "# dropout_rate = 0.8           # 드롭아웃 비율\n",
        "# l2_lambda = 0.001             # L2 정규화 값\n",
        "# patience = 7                  # 조기 종료를 위한 patience\n",
        "# min_delta = 0.001             # 조기 종료를 위한 최소 손실 감소량\n",
        "# validation_split = 0.2        # 검증 데이터 비율\n"
      ],
      "metadata": {
        "id": "6P298qHAdgIu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer=AdamW(learning_rate=learning_rate)\n",
        "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "H5sjHEL7d_NO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "39re5JFVLIYu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "46edf666-c29b-467d-bfa3-8400de80627b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 75ms/step - accuracy: 0.3071 - loss: 3.5400 - val_accuracy: 0.6488 - val_loss: 1.5889\n",
            "Epoch 2/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - accuracy: 0.7400 - loss: 1.1682 - val_accuracy: 0.7841 - val_loss: 0.9442\n",
            "Epoch 3/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 59ms/step - accuracy: 0.8680 - loss: 0.5267 - val_accuracy: 0.8024 - val_loss: 0.9384\n",
            "Epoch 4/10\n",
            "\u001b[1m412/412\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - accuracy: 0.9115 - loss: 0.3406 - val_accuracy: 0.8087 - val_loss: 0.9881\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d3bb044173ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# history = model.fit(\n",
        "#     X_train,\n",
        "#     y_train,\n",
        "#     epochs=epochs,\n",
        "#     batch_size=batch_size,\n",
        "#     validation_split=validation_split,\n",
        "#     callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_training_history(history)"
      ],
      "metadata": {
        "id": "hm5kEkgNZNrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N7z6VIrYaKH"
      },
      "outputs": [],
      "source": [
        "# 모델 전체를 저장하는 코드 (모델 아키텍처, 가중치, 컴파일 정보 포함)\n",
        "model.save('/content/drive/MyDrive/코딩공부/dacon_project/result/bbc_edit_model.h5')  # 모델을 'model.h5' 파일로 저장\n",
        "\n",
        "print(\"모델이 'bbc_edit_model.h5'로 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEHHGtG0PT13"
      },
      "outputs": [],
      "source": [
        "accr = model.evaluate(X_test,y_test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjjAF5w9jMTD"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/train.csv')\n",
        "# category_counts = df['분류'].value_counts()\n",
        "\n",
        "# # 결과를 출력합니다.\n",
        "# print(category_counts[:40])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzhPTfE-PfKs"
      },
      "outputs": [],
      "source": [
        "txt = [\"용인특례시, 가을 신학기 학교 급식 공급업체 위생점검\"]\n",
        "seq = tokenizer.texts_to_sequences(txt)\n",
        "padded = pad_sequences(seq, maxlen=max_len)\n",
        "pred = model.predict(padded)\n",
        "labels = [\n",
        "    \"지역\", \"경제:부동산\", \"사회:사건_사고\", \"경제:반도체\", \"사회:사회일반\", \"사회:교육_시험\",\n",
        "    \"정치:국회_정당\", \"사회:의료_건강\", \"경제:취업_창업\", \"스포츠:올림픽_아시안게임\",\n",
        "    \"경제:산업_기업\", \"문화:전시_공연\", \"경제:자동차\", \"경제:경제일반\", \"사회:장애인\",\n",
        "    \"스포츠:골프\", \"정치:선거\", \"경제:유통\", \"IT_과학:모바일\", \"사회:여성\",\n",
        "    \"사회:노동_복지\", \"사회:환경\", \"경제:서비스_쇼핑\", \"경제:무역\", \"정치:행정_자치\",\n",
        "    \"국제\", \"문화:방송_연예\", \"스포츠:축구\", \"경제:금융_재테크\", \"정치:청와대\",\n",
        "    \"문화:출판\", \"IT_과학:IT_과학일반\", \"IT_과학:인터넷_SNS\", \"문화:미술_건축\",\n",
        "    \"정치:정치일반\", \"IT_과학:과학\", \"문화:문화일반\", \"문화:학술_문화재\", \"문화:요리_여행\",\n",
        "    \"경제:자원\", \"문화:종교\", \"IT_과학:콘텐츠\", \"사회:미디어\", \"사회:날씨\",\n",
        "    \"스포츠:농구_배구\", \"문화:음악\", \"문화:생활\", \"IT_과학:보안\", \"스포츠:월드컵\",\n",
        "    \"경제:증권_증시\", \"정치:북한\", \"정치:외교\", \"스포츠:스포츠일반\", \"문화:영화\",\n",
        "    \"스포츠:야구\", \"경제:외환\"\n",
        "]\n",
        "# print(pred, labels[np.argmax(pred)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgikRQGtYAYO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. test.csv 파일 로드\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/test.csv')\n",
        "\n",
        "# 2. 데이터 전처리: 제목을 시퀀스로 변환\n",
        "# 학습 시 사용했던 tokenizer를 사용하여 제목을 시퀀스로 변환\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['제목'].values)\n",
        "\n",
        "# 3. 시퀀스의 길이를 동일하게 맞추기 (max_len에 맞춰 패딩)\n",
        "X_test = pad_sequences(sequences_test, maxlen=max_len)\n",
        "\n",
        "# 4. 모델을 사용하여 예측\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# 5. 예측 결과에서 가장 확률이 높은 클래스를 추출\n",
        "predicted_labels = predictions.argmax(axis=1)\n",
        "predicted_labels[predicted_labels>45]=0\n",
        "\n",
        "predicted_class_names=[categories[label] for label in predicted_labels]\n",
        "\n",
        "# 6. submission 데이터프레임 생성 (id와 예측된 레이블을 저장)\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': test_df['ID'],\n",
        "    '분류': predicted_class_names\n",
        "})\n",
        "\n",
        "# 7. submission.csv 파일로 저장\n",
        "submission_df.to_csv('/content/drive/MyDrive/코딩공부/dacon_project/result/submission.csv', index=False)\n",
        "\n",
        "print(\"Submission 파일이 생성되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFcqovMmgtWY"
      },
      "outputs": [],
      "source": [
        "df_test=pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/data/test.csv')\n",
        "df_result=pd.read_csv('/content/drive/MyDrive/코딩공부/dacon_project/result/submission.csv')\n",
        "print(\"Shpe of test Data\",df_test.shape)\n",
        "print(\"Shpe of test Data\",df_result.shape)\n",
        "# df_test.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsmrlMh8k2PB"
      },
      "outputs": [],
      "source": [
        "df_result.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUhmLt9dk4Ih"
      },
      "outputs": [],
      "source": [
        "print(df_result['분류'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_result['분류'])"
      ],
      "metadata": {
        "id": "9wMlcQiKxSqy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNEsv4/eKAC/ClRnbcMK/6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}